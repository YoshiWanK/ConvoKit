{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVeLQ2OcHN_2"
      },
      "source": [
        "# Converting the Interview 2P Dataset into the ConvoKit Format\n",
        "\n",
        "This notebook helps constructing a Convokit-formatted version of the dataset originally distributed with the following paper:\n",
        "\n",
        "Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian McAuley. 2020. [Interview: Large-Scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding](https://www.aclweb.org/anthology/2020.emnlp-main.653). In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 8129â€“41.\n",
        "\n",
        "Please cite this paper when using this corpus in your research.\n",
        "\n",
        "\n",
        "**Main Contributors:** Andrea Wang, Lucy Jiang, and Rebecca Hicke\n",
        "\n",
        "**Conversion Notebook Contributors:** Andrea Wang, Lucy Jiang, Rebecca Hicke, Yash Chatha, Sean Zhang\n",
        "\n",
        "**Original Dataset:** [NPR Interview 2P](https://www.kaggle.com/datasets/shuyangli94/interview-npr-media-dialog-transcripts?select=utterances-2sp.csv)\n",
        "\n",
        "Guide informed and inspired by:\n",
        "* [Converting the Cornell Movie-Dialogs Corpus into ConvoKit format](https://github.com/CornellNLP/ConvoKit/blob/master/examples/converting_movie_corpus.ipynb)\n",
        "* [ConvoKit Tutorial](https://colab.research.google.com/drive/1_jvL1t9PA2dERKbEm9pCnBS0sbW7B1AW?usp=sharing#scrollTo=kRu1nFlV4z-Z)\n",
        "\n",
        "We use the following files to create our Corpus:\n",
        "* **utterances.csv** contains 105k+ multi-party interview transcripts from 20 years of NPR interviews\n",
        "* **utterances-2sp.csv** contains all conversations within utterances.csv that are between two participants\n",
        "* **episodes.csv** contains the titles and program names for all episodes\n",
        "* **host-map.json** contains a dictionary of host ID: name (lowercase string), a list of episodes hosted, and a list of programs hosted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKcDYjxmJK6d"
      },
      "source": [
        "## Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xuSMlBiolHL"
      },
      "outputs": [],
      "source": [
        "!pip install convokit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7QU0FH3pQva"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from convokit import Corpus, Speaker, Utterance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcbbY1X0EM-8"
      },
      "source": [
        "### Data Import\n",
        "\n",
        "We uploaded and imported the dataset from Google Drive. The original dataset can be found on [Kaggle](https://www.kaggle.com/datasets/shuyangli94/interview-npr-media-dialog-transcripts?select=utterances-2sp.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wBGelOKERb5",
        "outputId": "d1432e9c-9579-42ac-eac7-bfde8631bc71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# For Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/Assignment 1 Group Project/dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAvrNyrCGZcF"
      },
      "outputs": [],
      "source": [
        "utterances2p = pd.read_csv(data_dir + \"utterances-2sp.csv\")\n",
        "utterances = pd.read_csv(data_dir + \"utterances.csv\")\n",
        "episodes = pd.read_csv(data_dir + \"episodes.csv\")\n",
        "episodes2p = utterances2p['episode'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYYA7iRMLrYJ",
        "outputId": "f93bea91-6bba-4a96-fcbb-f989df5f9f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 23714 2p episodes in this dataset.\n"
          ]
        }
      ],
      "source": [
        "print(f\"There are {episodes2p.size} 2p episodes in this dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlyKCadROrJw"
      },
      "source": [
        "### Data Cleaning\n",
        "\n",
        "We found the subset of episodes in **utterances.csv** that were between two participants as each row consisted of a full turn as opposed to the single sentence utterances in **utterances-2sp.csv**. Additionally, we removed all instances of utterances that were assigned to \"_NO_SPEAKER\", as these were transcriptions of non-dialogue sounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xStYD1WMBZP"
      },
      "outputs": [],
      "source": [
        "utterances = utterances[utterances['episode'].isin(episodes2p)]\n",
        "utterances = utterances[utterances['speaker'] != '_NO_SPEAKER']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SJZ04NZQksp"
      },
      "source": [
        "Some rows in **utterances-2sp.csv** contained incorrect encodings for *host_id* and *is_host*. This often took the form of (1) hosts being miscoded: when *host_id* was -1 for both participants, despite -1 being the proper value for a guest, or (2) guests being miscoded: when guests were given a *host_id* that was not -1. \n",
        "\n",
        "For the first issue, we found all episodes where the sum of *host_id* for all rows was equivalent to the number of turns taken within an episode multiplied by -1 (meaning every row within this episode had -1 in the *host_id* column), and removed these episodes from the dataset. For the second situation, we removed all episodes where the minimum *host_id* was not -1. We also removed all utterances with a null value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56rx9UqNSA73"
      },
      "outputs": [],
      "source": [
        "utterances2p_by_ep = utterances2p.groupby(['episode'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoiiuUUKXGQL"
      },
      "outputs": [],
      "source": [
        "# remove episodes where hosts are mis-coded\n",
        "x = utterances2p_by_ep.agg({\"episode\": \"size\", \"host_id\": \"sum\"})\n",
        "remove_episode = x[x['episode'] == -1*x['host_id']].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8w5avT5I2zw"
      },
      "outputs": [],
      "source": [
        "# remove episodes where guests are mis-coded\n",
        "y = utterances2p_by_ep.agg({\"host_id\": \"min\"})\n",
        "remove_episode = remove_episode.append(y[y['host_id'] != -1].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAa_wMJZXon3"
      },
      "outputs": [],
      "source": [
        "utterances = utterances[~utterances['episode'].isin(remove_episode)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntesKgiLyeVd",
        "outputId": "67d19c6f-2c14-411a-a445-997df54aa2a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1529 problematic 2p episodes that we have removed from this dataset.\n"
          ]
        }
      ],
      "source": [
        "print(f\"There are {len(remove_episode)} problematic 2p episodes that we have removed from this dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rYuHq8-QyNJ"
      },
      "source": [
        "Lastly, we removed all episodes where hosts were not included in **host-map.json** by finding the maximum *host_id* in each episode and comparing it against the list of hosts in **host-map.json**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDog4hkGQxnv"
      },
      "outputs": [],
      "source": [
        "# find all hosts represented in host-map.json\n",
        "host_map = json.load(open(data_dir + \"host-map.json\", \"r\"))\n",
        "hosts = pd.DataFrame.from_dict(host_map, orient='index')\n",
        "hosts = hosts.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODdUESTAToHD"
      },
      "outputs": [],
      "source": [
        "# remove episodes where hosts are not in host-map.json\n",
        "z = utterances2p_by_ep.agg({\"host_id\": \"max\"})\n",
        "json_hosts = hosts['index'].astype(int).to_list()\n",
        "missing_hosts = z[~z['host_id'].isin(json_hosts)].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9zLwuNWZnfV"
      },
      "outputs": [],
      "source": [
        "utterances = utterances[~utterances['episode'].isin(missing_hosts)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZL_OnRqqCnH",
        "outputId": "5d10ad8d-c47f-48e3-847b-1b32c5c93223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 775 episodes with hosts that are not in host-map.json that we have removed from this dataset.\n"
          ]
        }
      ],
      "source": [
        "print(f\"There are {len(missing_hosts)} episodes with hosts that are not in host-map.json that we have removed from this dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYKDJSInNS0P",
        "outputId": "a99e715f-8ee5-4ddd-fbe7-87a1cbf1c040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 22149 2p episodes left in this dataset.\n"
          ]
        }
      ],
      "source": [
        "print(f\"There are {utterances['episode'].unique().size} 2p episodes left in this dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYEiqyWt0d2p"
      },
      "outputs": [],
      "source": [
        "# remove null utterances\n",
        "utterances = utterances[utterances['utterance'].notnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JspAPlKg04K-",
        "outputId": "2fe9d743-9a4e-48ba-cde5-c16fe2d36b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 428624 utterances remaining in this dataset.\n"
          ]
        }
      ],
      "source": [
        "print(f\"There are {len(utterances)} utterances remaining in this dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BNLh9TG6X-2"
      },
      "source": [
        "## Create Speakers\n",
        "\n",
        "We begin by determining which speaker is the host in a given conversation. While most host labels have the word \"host\" in the name, this is not consistent across the entire dataset. We identify which speakers are hosts by aggregating individual utterances in **utterances-2sp.csv** such that they represent each turn. This then allows us to map each *host_id* value in **utterances2p.csv** to its corresponding turn in **utterances.csv**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KybVPvjVxxUf"
      },
      "outputs": [],
      "source": [
        "# use utterances2p to find host_id (rather than use \"host\" in speaker_name)\n",
        "utterances2p = utterances2p[~utterances2p['episode'].isin(remove_episode)]\n",
        "episode_order2host_id = utterances2p.groupby([\"episode\", \"episode_order\"]).agg({\"host_id\": \"min\"}).reset_index()\n",
        "utterances = utterances.merge(episode_order2host_id, how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNO6rvhiC4z5"
      },
      "source": [
        "We then assign guests a *speaker_id* based on the episode that they are part of. For simplicity, we treat each guest as a separate speaker (e.g.: even if the same guest appears in two different episodes, they are still assigned two different IDs). We determine which speaker is a guest by utilizing the utterances-2sp.csv file, in which guests are identified with *host_id* = -1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK-4bEU1Nxc7"
      },
      "outputs": [],
      "source": [
        "# create guest speaker_id\n",
        "utterances['speaker_id'] = utterances.apply(lambda row: \"g\" + str(row['episode']) if row['host_id'] == -1 else \"\", axis=1)\n",
        "\n",
        "# create host speaker_id\n",
        "utterances['speaker_id'] = utterances.apply(lambda row: \"h\" + str(row['host_id']) if row['host_id'] != -1 else row['speaker_id'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noxNfP_X2hNH",
        "outputId": "498aa35a-5d32-43a2-bbd3-5a09a045707c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "episode\n",
              "1         2\n",
              "97596     2\n",
              "97593     2\n",
              "97590     2\n",
              "97589     2\n",
              "         ..\n",
              "63680     2\n",
              "63679     2\n",
              "63674     2\n",
              "63700     2\n",
              "141179    2\n",
              "Name: speaker_id, Length: 22149, dtype: int64"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sanity check - each episode should have exactly two speakers\n",
        "utterances.groupby([\"episode\"])['speaker_id'].nunique().sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms5tm2Z63oze"
      },
      "source": [
        "### Assign Speaker Metadata\n",
        "\n",
        "We gather host names from *host-map.json* to omit the \", host\" tag that is present in the speaker names in **utterances.csv**. For each speaker, we save their name and the type of speaker that they are (host or guest). We then create `Speaker` objects for each host and guest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfzqDa6kj8Fv"
      },
      "outputs": [],
      "source": [
        "speaker_meta = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFvZGcGPVQLw"
      },
      "outputs": [],
      "source": [
        "# create host data from host_map\n",
        "hosts['speaker_id'] = \"h\" + hosts['index']\n",
        "hosts2p = hosts[hosts['speaker_id'].isin(utterances['speaker_id'].unique())]\n",
        "hosts2p = hosts2p[['name', 'speaker_id']].to_dict(orient='records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnknwF3_4Qn1"
      },
      "outputs": [],
      "source": [
        "for host in hosts2p:\n",
        "  speaker_meta[host['speaker_id']] = {\"name\": host['name'], \"type\": \"host\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYOrkhLo3UXP"
      },
      "outputs": [],
      "source": [
        "# create guest data from utterances.csv\n",
        "speakers = utterances[['speaker', 'speaker_id']].drop_duplicates()\n",
        "guests = speakers[speakers['speaker_id'].str.startswith(\"g\")].to_dict(orient='records')\n",
        "for guest in guests:\n",
        "  speaker_meta[guest['speaker_id']] = {\"name\": guest['speaker'], \"type\": \"guest\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTAosAV3y2Kd"
      },
      "outputs": [],
      "source": [
        "corpus_speakers = {k: Speaker(id = k, meta = v) for k,v in speaker_meta.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jizmbczG6f8z"
      },
      "source": [
        "## Create Utterances and Corpus\n",
        "\n",
        "To create `Utterance` objects, we iterate through the DataFrame of **utterances** to capture the reply structure and metadata (*episode*, and *order*). As two-participant interviews are linear in structure, we consider all utterances following the first in a conversation to be a reply. We identify new conversations by tracking the last *episode* that corresponds to each utterance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFAdkwTMVm-_"
      },
      "outputs": [],
      "source": [
        "utterances = utterances.reset_index(drop=True)\n",
        "utterances = utterances.sort_values(['episode', 'episode_order'])\n",
        "\n",
        "utterance_corpus = {}\n",
        "root = \"\"\n",
        "last_episode = -1\n",
        "\n",
        "for index, utterance in utterances.iterrows():\n",
        "  if utterance[\"episode\"] != last_episode:\n",
        "    root = str(index)\n",
        "    reply_to = None\n",
        "  else:\n",
        "    reply_to = str(index-1)\n",
        "  meta = {\"episode\": utterance[\"episode\"], \"order\": utterance[\"episode_order\"]}\n",
        "  utterance_corpus[index] = Utterance(id = str(index), speaker = corpus_speakers[utterance[\"speaker_id\"]], text = str(utterance[\"utterance\"]), root = root, reply_to = reply_to, meta = meta)\n",
        "  last_episode = utterance[\"episode\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ_OYp5inZl8"
      },
      "source": [
        "Lastly, we create the `Corpus` from a list of `Utterance`s. Each `Conversation` contains metadata including fields such as *program*, *title*, and *date*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RaEpF_naoNa"
      },
      "outputs": [],
      "source": [
        "utterance_list = utterance_corpus.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq0PSIJRavSx"
      },
      "outputs": [],
      "source": [
        "corpus = Corpus(utterances = utterance_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0caOZRv8g0Zw"
      },
      "outputs": [],
      "source": [
        "episodes = episodes[episodes[\"id\"].isin(episodes2p)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFCBvJXWgLVn"
      },
      "outputs": [],
      "source": [
        "ep_info_dict = {}\n",
        "for index, ep in episodes.iterrows():\n",
        "  ep_info_dict[ep[\"id\"]] = {\"program\": ep[\"program\"], \"title\": ep[\"title\"], \"date\": ep[\"episode_date\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TprusheSeKyL"
      },
      "outputs": [],
      "source": [
        "for convo in corpus.iter_conversations():\n",
        "  convo_id = convo.get_id()\n",
        "  utt = convo.get_utterance_ids()[0]\n",
        "  episode_id = corpus.get_utterance(utt).meta[\"episode\"]\n",
        "  convo.meta.update(ep_info_dict[episode_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0lhhG-wFZM_"
      },
      "source": [
        "## Save Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESsL8lyuFYVd"
      },
      "outputs": [],
      "source": [
        "corpus.dump(\"npr-2p-corpus\", base_path = data_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
